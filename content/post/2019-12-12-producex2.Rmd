---
title: 텍스트분석을 통한 프로듀스X101 데뷔조 예측1 [데이터수집]
author: JDW
date: '2019-12-12'
slug: producex2
categories:
  - 데이터분석
tags:
  - R
output:
  blogdown::html_page:
    toc: true
---

```{r, message=FALSE, include=FALSE}
library(tidyverse)
library(data.table)
library(knitr)

```


# 데이터 수집 

데이터 분석을 위해 가장 먼저 해야 할 일은 분석의 대상이 될 데이터를 확보 하는 일 입니다. 

이전 글에서 말했다시피 이번 파트에서 진행할 것은 데이터 크롤링에 관한 전반적인 것들 입니다.

프로듀스X101 관련 모든 커뮤니티의 글들을 수집하는것은 물리적으로 불가능에 가깝기 때문에 해당방송과 관련된 커뮤니티중 가장 규모가 큰 한곳을 선정하여 데이터 수집을 진행하였습니다.
<br>
<br>

![](/post/2019-12-12-producex2_files/dc_producexgallery.png){width=80%}
<center>

<em>디시인사이드 프로듀스x101 갤러리</em>
</center>

<br>

해당 커뮤니티는 방송이 인기를 끌었던 시점에 하루에 대략  4만건의 게시물이 올라왔었는데요. 
한참 방송이 진행중이던 06/01 부터 마지막 생방송날인 07/19 까지의 데이터를 수집하였었습니다. 
<br>

웹 데이터 수집을 위한 여러 방법이 있지만 R 에서 웹페이지를 크롤링할때 보편적으로 사용하는 `rvest` 패키지를 활용하여 크롤링을 진행하였습니다.
<br>

아래는 크롤링 코드 입니다. 


# code

```{r, eval = F}
# 저장 폴더 설정 
setwd('./PRODUCEX/CRAWLING DATA')

library(rvest)
library(dplyr)
library(lubridate)
library(stringr)
```

<br>

먼저 크롤링한 데이터를 저장할 폴더와 필요한 패키지들을 호출 합니다. 

<br>

```{r t1, eval = F}
basic_url <- 'https://gall.dcinside.com/board/lists/?id=producex&page='

# 주소설정 ----

urls <- NULL
for(x in 0:599){
    urls[x + 1] <- paste0(basic_url, x + 1)
}
```

<br>

웹 데이터를 크롤링해 오는데엔 두가지 방식이 있습니다. 바로 GET 방식과 POST방식인데요.  <br>
GET 방식은 웹페이지의 URL주소를 기반으로 데이터를 가저오는 형식입니다. 즉, 특정한 웹페이지의 주소를 서버측에 요청하면 서버는 요청에 대한 응답으로 웹페이지 URL의 정보를 클라이언트(사용자)에게 보내주는 것입니다. 우리가 인터넷을 사용할때 주소창에 URL주소를 입력하고 엔터를 누를때 웹페이지가 변경되는 것을 생각하시면 됩니다. 

R에서는 이러한 동작의 수행을 위해서 `rvest` 패키지를 보편적으로 사용합니다. 

<br>

웹페이지 주소를 기반으로 정보를 가져오기 때문에 정보를 가져올 웹페이지의 주소를 for문을 통해서 만들어 줍니다. 

해당 코드를 돌리면 다음과 같은 결과값이 나옵니다. 

```{r t2, warning=FALSE, message=FALSE, echo = FALSE}

basic_url <- 'https://gall.dcinside.com/board/lists/?id=producex&page='
urls <- NULL
for(x in 0:599){
    urls[x + 1] <- paste0(basic_url, x + 1)
}

head(urls)

```

```{r t3, warning=FALSE, message=FALSE, echo = FALSE}

basic_url <- 'https://gall.dcinside.com/board/lists/?id=producex&page='
urls <- NULL
for(x in 0:599){
    urls[x + 1] <- paste0(basic_url, x + 1)
}

tail(urls)

```

<br>

for문을 통해 1페이지부터 600페이지까지 총 600개의 웹페이지 주소가 생성되었네요. 

만들어진 주소들을 기반으로 크롤링을 진행합니다. 먼저 크롤링 정보를 담을 빈객체들을 생성합니다. 

<br>

```{r, eval = F}

# 크롤링 ----

html1 <- NULL
p_title <- NULL
p_writer <- NULL
p_writer_id <- NULL
p_time <- NULL
p_count <- NULL
p_recommend <- NULL
dc <- NULL

```

<br>

본격적으로 for문을 통해 데이터를 페이지순으로 순차적으로 가져옵니다. 

<br>

```{r, eval = F }

for(url in urls){
            html1 <- read_html(url)
            p_title <- c(p_title, html1 %>%
                             html_nodes('.ub-content.us-post') %>%
                             html_nodes('.gall_tit.ub-word') %>%
                             html_node('a') %>%
                             html_text())
            p_writer <- c(p_writer, html1 %>%
                              html_nodes('.ub-content.us-post') %>%
                              html_nodes('.gall_writer.ub-writer') %>%
                              html_attr('data-nick'))
            p_writer_id <- c(p_writer_id, html1 %>%
                                 html_nodes('.ub-content.us-post') %>%
                                 html_nodes('.gall_writer.ub-writer') %>%
                                 html_attr('data-uid'))
            p_time <- c(p_time, html1 %>%
                            html_nodes('.ub-content.us-post') %>%
                            html_nodes('.gall_date') %>%
                            html_attr('title'))
            p_count <- c(p_count, html1 %>%
                             html_nodes('.ub-content.us-post') %>%
                             html_nodes('.gall_count') %>%
                             html_text())
            p_recommend <- c(p_recommend, html1 %>%
                                 html_nodes('.ub-content.us-post') %>%
                                 html_nodes('.gall_recommend') %>%
                                 html_text())

}
```


<br>

해당 for문을 돌릴시 R은 맨 처음 만들었던 주소들 즉, urls 객체에 담긴 주소값을 받아서 서버에게 순차적으로 주소에 대한 정보를 요청하게 됩니다. 그리고 for문 내부에 있는 코드를 수행하는데요, 각각의 객체에 해당하는 값들을(p_title, p_time 등등..) 파싱합니다. 모든 코드를 수행하면 for문은 다음 주소를 요청하여 처음 코드부터 다시 파싱을 진행하게 됩니다.


<br>

```{r, eval = F}

# 데이터 프레임화 
dc <- data.frame(p_title, p_writer, p_writer_id, p_time, p_count, p_recommend)

# 날짜조정 ----
dc$p_time <- ymd_hms(dc$p_time)
dc <- dc[day(dc$p_time) == day(today()-1), ]

```

<br>

for문을 통해 수집한 각각의 객체 데이터들을 'dc' 라는 이름의 데이터프레임에 담아줍니다. 시간을 다루는 `lubridate` 패키지의 `ymd_hms()`함수를 이용하여 글 작성 시간을 의미하는 `p_time` 데이터를 시간 데이터로 변경해 줍니다. 

저는 이 코드를 `taskscheduleR` 패키지를 활용하여 매일 자정무렵 자동으로 돌아가게끔 설정 하였었는데요, 데이터를 매일 수집하는 것이다 보니 오직 전날에 작성된 글들만을 저장하기 위해서 `dc <- dc[day(dc$p_time) == day(today()-1), ]` 필터링 코드를 만들었습니다.  코드를 짧막하게 설명하자면 글 작성 시간을 의미하는 `dc$p_time` 이 수집하는 날 바로 하루 전 `today() - 1` 의 값과 동일한 값만을 dc객체로 새로이 담아낸 것입니다. 

```{r echo = FALSE, results = 'asis'}

dc <-fread('C:/Users/JDW/Desktop/my_cloud/OneDrive/R/producex/data/2019-06-1.csv')
kable(tail(dc[,c(1, 3:8)]))
```

<center>
<em>크롤링을 마친 데이터의 구조 </em>
</center>

<br>

```{r eval = FALSE}
# 파일저장 ----
write.csv(dc, paste0(ymd(today()-1), ".csv"))
```

<br>

마지막으로 필터링까지 완료된 데이터를 csv파일로 저장합니다. 













